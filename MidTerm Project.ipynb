{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMS MIDTerm Project: Statistical and ML Analysis of the \"Titanic\" dataset.\n",
    "### By Dammaris Abril Barrera Moreno.\n",
    "\n",
    "\n",
    "## 1. **Introduction**\n",
    "\n",
    "The porpuse of the activity is to analyze the titanic dataset using \"Orange\" as a tool. Also perform some activities in the set using python.Since it is my first time using orange, I tried all the options all could in order to familiarize myself with the software, using the Titanic dataset of course. In order to make a final conclusion of there are specific factors that influenciate the survival range of the persons inside the ship, I will use distributions, plots, as other visualization tools.  \n",
    "![Orange testing using Titanic Dataset](https://raw.githubusercontent.com/Abril-Barrera/Smart-Business-Systems/master/all.PNG)\n",
    "\n",
    "\n",
    "### **Problem Definition**\n",
    "\n",
    "The Titanic was a passenger ship that sank in the North Atlantic Ocean in April 1912 after colliding with an iceberg during its first travel from Southampton to New York City. It had an estimated of 2,224 passengers + the workers of the ship. More than 1,500 died in this accident and that is why this is a very famous case of study nowadays. It is often considered as the “hello world” of the data analysis, since it is worldwide used to get into the matter.\n",
    "\n",
    "The idea is to find out if there were any factors that influenciated in the survival or death of the passengers as well as determine them (in case there are), but just by the previous look at the data I am starting to diverge into the hipotesis that the statement is true. I also expect the age, gender and class type variables to be very important just by the logical approach of the problem, but I will have to prove is that is correct with the adequate information.\n",
    "\n",
    "## 2. **Dataset Description **\n",
    "\n",
    "The Titanic dataset contains several info about the passengers age, sex, ticket class, etc. As well as the survival value.It is composed by these 12 variables as shown in Orange while loading the CVS file.\n",
    "\n",
    "![Variables of the titanic dataset](https://raw.githubusercontent.com/Abril-Barrera/Smart-Business-Systems/master/eda1.PNG)\n",
    "\n",
    "#### __Description of each variable__\n",
    "\n",
    "_survival:    Survival  (0= died 1=survived)\n",
    "PassengerId: Unique Id of a passenger. \n",
    "pclass:    Ticket class  (1=1st, 2=2nd, 3=3rd class tickets)\n",
    "sex:    Sex     \n",
    "Age:    Age in years     \n",
    "sibsp:    # of siblings / spouses aboard the Titanic     \n",
    "parch:    # of parents / children aboard the Titanic     \n",
    "ticket:    Ticket number     \n",
    "fare:    Passenger fare     \n",
    "cabin:    Cabin number     \n",
    "embarked:    Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)_\n",
    "\n",
    "### **Justification of analysis**\n",
    "\n",
    "It is possible to make different approaches to the data. The suggestion was to make an exploration whether there were certain factors that causes if someone survived the catastrophe of \"Titanic\" or not. For this one, we could take into a consideration vairables as class or age. Since they could give me a lead on this question and help me create an hipotesis. However I am going to be doing different analysis and visualizations while trying to find the most interesting comparitions. As well as trying the different tools Orange offers in order to analyze data.\n",
    "\n",
    "The main focus as I said before will be to find the biggest factors in the person's survival rate. Because of this I will select the survival variable as my target. As well as clasify the name, ticket and cabin as metadata variables. First of all, I am going to visualize the general data by using the \"Data Table\" option in orange. Since the beginning I notice some very important details as there are no missing values in the survival variable, that there is a rate of missing values of 2.5% in the feature values and a 25.7% in the metadata ones.\n",
    "\n",
    "![Titanic Dataset](https://raw.githubusercontent.com/Abril-Barrera/Smart-Business-Systems/master/eda2.PNG)\n",
    "\n",
    "\n",
    "## 3. **Exploratory Analysis**\n",
    " \n",
    "\n",
    " By having a quick look to the data, at first I see that all the columns make sense, even though I thing some of them will be more useful. The scale of the values is correct and I don't think missing data will be a problem for the most of the features. Even so, for the feature \"CABIN\" it is a big problem since most of the values are missing, so I'll definitely will have to clean that. Also, the age feature has a decent amount of missing values. \n",
    " \n",
    " I tried all kind of plots in order to find patrons and find the correct algorithms to use. The most helpful option was definitely the box plot. The box plot allowed me to visualize the quartiles and in consecuence where most part of the passengers population was in. Also the median and the mean, in order to see the tendency of the information.\n",
    " \n",
    " 1. Box Plot, fare, survived. _Through this plot, I obteined the mean of both cases in the survival variable. Being the mean of fare the the survivals more the **double** of the people who died, making me think that the people that were richer were most likely to survive. \n",
    " \n",
    " ![Box Plot, fare, survived](https://raw.githubusercontent.com/Abril-Barrera/Smart-Business-Systems/master/plot2.PNG)\n",
    " \n",
    " 2. Scatter plot of Age, Pclass and survival. _I can see a clear tendency looking at this graph, the survival rate increases as equal as the class. In other words, the first class people survived the most. Most people who survived and where in 3rd and 2nd class were young ones. When it comes to the first class, people of all kind of ages survived but most of them are in a range between 15-55 years._\n",
    " \n",
    " ![Scatter plot of Age, Pclass and survival variables](https://raw.githubusercontent.com/Abril-Barrera/Smart-Business-Systems/master/plot1.PNG)\n",
    " \n",
    " 3. Box plot of survival & embarked variable. _The people who embarked in Queenstown basically died and survive equally. While people who embarked in Southampton died most than survive (there isn't a big difference still). And the opposite is with the Cherbourg people (they survived the most), 2/3 of them survived._\n",
    " \n",
    " ![Box plot of survival & embarked variable](https://raw.githubusercontent.com/Abril-Barrera/Smart-Business-Systems/master/plot3.PNG)\n",
    " \n",
    " 4. Box plot survival and sex. _A clear tendency of sex influencing the survival. As the plot shows women survived much more than men. Since 70% of the survivors were women, the famous saying of the titanic \"women and children first\" was definitely a true statement._\n",
    "  \n",
    " ![Box plot sex and survival](https://raw.githubusercontent.com/Abril-Barrera/Smart-Business-Systems/master/plot4c.PNG)\n",
    " \n",
    " 5. Box plot age and survival. _Most of the survivals are around 19-36 years old, 25% of them are kids and teenagers (less than 18 or less). Apparently almost no kids died in the incident since the first quartile of the deads starts from 17 years old.\n",
    " \n",
    " ![Box plot sex and survival](https://raw.githubusercontent.com/Abril-Barrera/Smart-Business-Systems/master/plot5cc.PNG)\n",
    " \n",
    " * Distributions\n",
    " \n",
    " 1. Normal distribution Age/Frequency 5 Width. _The general population on the ship were in the range of 0-80 years, but using the normal distribution we can see the bell goes from 0 -60 years, so normally I consider the rest as “outliers”. Even so, I’m not removing any kind of data of the dataset since I don’t consider it appropriate in this case._\n",
    " \n",
    "  ![Normal distribution Age/Frequency 5 Width](https://raw.githubusercontent.com/Abril-Barrera/Smart-Business-Systems/master/dis1.PNG)\n",
    " \n",
    " 2. Distribution by parch-survived. _The data is very upfront when it comes to having 2 or mor parents or children in the ship, there isn’t a difference at all. When it comes to having 1 related then it seems like the possibility to do not survive increased but the results are very inconclusive since there is a very small tendency._\n",
    " \n",
    " ![Distribution by parch-survived](https://raw.githubusercontent.com/Abril-Barrera/Smart-Business-Systems/master/dis2.PNG)\n",
    " \n",
    " \n",
    " 3. Normal distribution by SibSp-survived. _Again, I don’t think the data is very conclusive, but there is a more clear tendency here, passengers that had 1 sibling or spouse survived more, but after that the results are very similar for the passengers who had more than one sibling or spouse in the ship._\n",
    " \n",
    " ![Normal distribution by SibSp-survived](https://raw.githubusercontent.com/Abril-Barrera/Smart-Business-Systems/master/dis3.PNG)\n",
    " \n",
    " \n",
    " 4. Normal distribution age-survived. _This distribution is very interesting for this dataset because it shows the bell of survivals and dead passengers. There are 2 ranges, the survival bell range is between 0-56 years old. While the non-survival one is from 0 to 60. A larger range makes sense because the amount of people who died is bigger than the survivors._\n",
    "  \n",
    "  ![Normal distribution age-survived](https://raw.githubusercontent.com/Abril-Barrera/Smart-Business-Systems/master/dis4.PNG)\n",
    " \n",
    "\n",
    "  * Discretizations\n",
    "  \n",
    "  \n",
    " 1. Discretization of age variable. _Using a discretization to visualize the data is another good tool. In this example I separated ages in 7 ranges. This allowed me to obtain very precise info. As confirming that the bigger survivor group were kids. As well as people that were between 30-36 years old. Also that 10% of the people how died in the accident were less than 16 years old and this was the smallest group of age._ \n",
    "  \n",
    "  ![Discretization of age variable](https://raw.githubusercontent.com/Abril-Barrera/Smart-Business-Systems/master/discret1.PNG)\n",
    "\n",
    " \n",
    "At the end there is a clear tendency that is appropiate for all the analysis I did. Women were the ones that survived the most (70% percent of the survivors), also most of the kids survived as well. After that, the people with a higher class (1st class people) were the ones that survived the most, followed by 2nd class ones. Most of the population were around 15-55 acording with the normal distribution. I didnt find any conclusive information about the parch and the sibsp variables causing a real effect on the survival rate, when it comes to having parents/children in the ship, the analysis is clear, it didn't matter at all. When it comes to siblings/spouses the results show a small tendency to having more than one sibling and not surviving, but as I said it is really small, I don't think is appropiate to conclude so based on the small tendency.\n",
    "\n",
    "I can say the most vulnerable group were: men, 3rd class, embarked on Southampton, were more than 18 years old, didn't have a sibling/spouse/parent/children or had 2 or more and payed a low fare. While the most survival group were: women, 1st class, embarked on Cherbourg, were between 0-36 years old, had 1 sibling/spouse/parent/children amd payed a high fare.\n",
    "\n",
    "So I can say, there were factors that actually made an impact in the survivals. The most important ones in order: gender, age and class. Variables as and fare, embarked mattered too but they are really related to the class one (economical status). The rest, I don't think are so reelevant but I still could infer some informations. The rest of variables were metadata so I didn't really use it at all in this analysis. \n",
    "\n",
    "### **Hipotesis**\n",
    "\n",
    "After doing the exploratory analysis I was able to formulate a hipotesis that I will be trying to prove by using stadistical analysis and the machine learning algorithms. This hipotesis is based on:\n",
    "The three most important features of the dataset (in order) are:\n",
    "\n",
    "1. Gender\n",
    "2. Pclass\n",
    "3. Age\n",
    "\n",
    "The is a very clear tendency when it comes to people who survived the tragedy and this is that they are mostly women, on the first class and were children or between 20-40. This is what all the exploratory information shows. So my purpose is going to be to prove my hipotesis correct. For this I am going to do the proper data preparation and cleaning in order to obtain the best results. I am almost sure I will be able to prove this theory thorugh the explanatory phase.\n",
    "\n",
    "\n",
    "## 4. **Data Preparation & Cleaning**\n",
    "\n",
    "The training set has initially 1 decision variable (survival) | 10 features (passengerId, pclass, sex, age, sibsp, parch, ticket, fare, cabin, embarked). By analyzing the data I see that the fare the passengert payed its actually kind of redundant with the pclass one. Hence, the pclass is categorical I will stick with that because I believe it can provide me more clear and consice information. When it comes to the \"sibsps\" and \"parch\" variable, I am not going to use those neither, the exploratory analysis was enough to determine these two won't have a big impact in the model and also I am not really interested in the information that they can provide me. \n",
    "\n",
    "Then, I am going to skip the ticket number, passengerID and the cabin two. The first one, because I already have the name and both of them are metadata refering to the passenger's identification information, it would be redundant to keep both for my purposes. And the cabin one is simple, there is an extremely big amount of missing values on this variables. The vaiable is not so important and even if it was, it is not worth keeping it since it can't provide me concise information since it will only allow me to evaluate a small sample. \n",
    "\n",
    "So these are the variables that I am going to keep: \n",
    "- [] name: name of passenger\n",
    "- [] survival:    Survival  (0= died 1=survived)\n",
    "- [x] <del>PassengerId: Unique Id of a passenger.</del> \n",
    "- [] pclass:    Ticket class  (1=1st, 2=2nd, 3=3rd class tickets)\n",
    "- [] sex:    Sex     \n",
    "- [] Age:    Age in years     \n",
    "- [x] <del>sibsp:    # of siblings / spouses aboard the Titanic</del>  \n",
    "- [x] <del>parch:    # of parents / children aboard the Titanic</del>    \n",
    "- [x] <del>ticket:    Ticket number</del>     \n",
    "- [x] <del>fare:    Passenger fare</del>     \n",
    "- [x] <del>cabin:    Cabin number </del>   \n",
    "- [] embarked:    Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)\n",
    "\n",
    "The table will now look like this: \n",
    "\n",
    "![Data table with updated variables](https://raw.githubusercontent.com/Abril-Barrera/Smart-Business-Systems/master/m1.PNG)\n",
    "\n",
    "Now I have to focus in cleaning the data. Look for missing values, outliers, duplicates, etc. As I noticed in my exploratory analisis, the age variable is going to be a problem. It has a lot of missing values and since it is really important for my model, I have to do something to fix that before working with the data. \n",
    "\n",
    "In order to fix this, I have a few options. One is filling the gaps with random values or with the most common one, I definitely don't want to do this because I would be corrupting my information (since age will be an important part of my model). The other option I have is to remove the observations that are missing the value. I definitely don't think this is a great approach either, since this dataset is composed by just **891** observations and the titanic actually had more than the double passanger. This means **I AM ALREADY MISSING A LOT OF DATA**, the less I want to do is to loss more of it. By doing this I would be losing 179 observations, what can misslead me to a correct assumption later on. \n",
    "\n",
    "Even so, after thinking about it for a while, I decided to remove the missing rows instead of filling it. Hopefully it won't affect my results that much. I am sure, there are greater methods to clean the data, but for now since I am a beginner I will stick with those. I will try to apply a further cleaning process for the final project. \n",
    "\n",
    "By removing the missing values observations on the age variable, I ended up with 714 observations. By looking for more missing values in the other variables I found out there are just 2 more of them, as a consecuence. I decided to clear all of them to make my dataset more relyiable. The final result is a training set with 712 observations.\n",
    "\n",
    "Data table with removed rows of the missing values (712 observations):\n",
    "\n",
    "![Data table with removed rows of the missing values](https://raw.githubusercontent.com/Abril-Barrera/Smart-Business-Systems/master/m2.PNG)\n",
    "\n",
    "It is important to mention that I did find outliers in the data but as I couldn't find a reason good enough to get rid of them, I think I could compromise the veracity of the information by doing so. Outliers in this sample are definitely not something to worry about.\n",
    "\n",
    "After the previous procedure I believe the dataset is fairly functional. Probably I could have lost less data and make some more small changes to make it \"perfect\" but since I am not a professional. I think if I go further I could make the mistake of overfitting the dataset, so I believe it is already decent enough to work with it. I believe the approaches of continuize and discretize the dataset are also good ones and I tried them myself in the exploratory phase but for now since I don't consider them necessary to the algorightms I am going to try in the following examples, I am going to skip them. If I believe it is necessary, I would use these approaches in a specific type of algorithm.\n",
    "\n",
    "This is how the dataset would look like with continuize data (my favorite approach for the titanic ds):\n",
    "\n",
    "![Data table with removed rows of the missing values](https://raw.githubusercontent.com/Abril-Barrera/Smart-Business-Systems/master/m3.PNG)\n",
    "\n",
    "This is how the dataset would look like with discretize data:\n",
    "\n",
    "![Data table with removed rows of the missing values](https://raw.githubusercontent.com/Abril-Barrera/Smart-Business-Systems/master/m4.PNG)\n",
    "\n",
    "Finally, after the cleaning process I will just separate the data set into the training data (80%) and the test data (20%). These percentages because of the recomendation for the models and machine learning algorithms. By doing so, these are my results: \n",
    " \n",
    "* TestSet =  143 observations.\n",
    "* TrainingSet = 569 observations.\n",
    "\n",
    "![Data table with removed rows of the missing values](https://raw.githubusercontent.com/Abril-Barrera/Smart-Business-Systems/master/m5.PNG)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. **Analisis (Estatistical & machine learning) **\n",
    "\n",
    "### - **Random Forest**\n",
    "\n",
    "The random forest algorithm combines multiple algorithm of the same type (decision trees), resulting in a forest of trees, hence the name \"Random Forest\". The random forest algorithm can be used for both regression and classification tasks.\n",
    "\n",
    "We should use the random forest algorithm because it has a lot of strengths. The first is that it is probably the most adaptable and easy to use alrogithm (great combination). It is highly accurate, this meaning it does not sacrifice capability because of its positive features. It performs random sampling and it is robust against overfitting. It is somewhat interpretable because you gan get relative feature importance. That means that while you dont know exactly how it came upt with its decision you know generally speaking which variables contributed more than others to the outcome. The only big weakness is that they can be slow to run. With large data sets it can be impractical because of its complexity.\n",
    "\n",
    "The following are the basic steps involved in performing the random forest algorithm:\n",
    "\n",
    "1. Pick N random observations.\n",
    "2. Build a decision tree based on these N records.\n",
    "3. Choose the number of trees you want in your algorithm and repeat steps 1 and 2.\n",
    "4. The final value is calculated with the average or most repited value.\n",
    "\n",
    "I am going to work with the sklearn library in order to use the RandomForest alrogithm that is provided there.\n",
    "This algorithm can provide me 3 pieces of information:\n",
    "\n",
    "- Confusion matrix\n",
    "- Classification report\n",
    "- Accuracy score\n",
    "\n",
    "#### Confusion matrix\n",
    "\n",
    "The confusion matrix is a tool that allows me to visualize the performance of an algorithm that is used in sueprvised learning. Each colum represents the number of predictions of each class, while each row represents the instances of the real class. One of the bennefits of this type of matrix is that facilitates to see if the system is confusing the different classes or results of the classification. We can see the structure of the matrix in the following picture:\n",
    "\n",
    "![Confusion matrix](https://raw.githubusercontent.com/Abril-Barrera/Smart-Business-Systems/master/m11.PNG)\n",
    "\n",
    "* VP is the amount of positives that were classified correctly as positives.\n",
    "* VN is the amount of negatives that were classified correctly as negatives.\n",
    "* FN is the amount of positives that were classified incorrectly as negatives.\n",
    "* FP is the amount of negatives that were classified incorrectly as positives.\n",
    "\n",
    "#### Classification report\n",
    "\n",
    "This report will contain the following paramerts.\n",
    "\n",
    "![Confusion matrix](https://raw.githubusercontent.com/Abril-Barrera/Smart-Business-Systems/master/m12.PNG)\n",
    "\n",
    "#### Accuracy score\n",
    "\n",
    "Very self explanatory, basically how accurate the prediction was. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pclass  Sex   Age  Embarked  Survived\n",
       "0       1    0  24.0       0.0         1\n",
       "1       1    0  54.0       0.0         1\n",
       "2       1    1  52.0       1.0         0\n",
       "3       1    0  18.0       0.0         1\n",
       "4       2    1  27.0       1.0         0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load dataset and display it\n",
    "\n",
    "import pandas as pd\n",
    "dataset = pd.read_csv('C:\\\\users\\\\Abril\\\\Downloads\\\\Smart business systems\\\\Titanic\\\\randomForest.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining features and the target (survival)\n",
    "\n",
    "X = pd.DataFrame(dataset.iloc[:,:-1])\n",
    "y = pd.DataFrame(dataset.iloc[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>37.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>143 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pclass  Sex   Age  Embarked\n",
       "0         1    0  24.0       0.0\n",
       "1         1    0  54.0       0.0\n",
       "2         1    1  52.0       1.0\n",
       "3         1    0  18.0       0.0\n",
       "4         2    1  27.0       1.0\n",
       "..      ...  ...   ...       ...\n",
       "138       3    0  45.0       0.0\n",
       "139       2    0  21.0       1.0\n",
       "140       1    1  37.0       1.0\n",
       "141       2    0  18.0       1.0\n",
       "142       3    1  32.0       1.0\n",
       "\n",
       "[143 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>143 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Survived\n",
       "0           1\n",
       "1           1\n",
       "2           0\n",
       "3           1\n",
       "4           0\n",
       "..        ...\n",
       "138         0\n",
       "139         1\n",
       "140         0\n",
       "141         1\n",
       "142         1\n",
       "\n",
       "[143 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate training and testing data sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=3, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=30,\n",
       "                       n_jobs=None, oob_score=False, random_state=1, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the random forest classifier function and build the model\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(n_estimators=30, criterion='gini', random_state=1, max_depth=3)\n",
    "classifier.fit(X_train,y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict values using the random forest classifier model\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11  2]\n",
      " [ 5 11]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.85      0.76        13\n",
      "           1       0.85      0.69      0.76        16\n",
      "\n",
      "    accuracy                           0.76        29\n",
      "   macro avg       0.77      0.77      0.76        29\n",
      "weighted avg       0.78      0.76      0.76        29\n",
      "\n",
      "0.7586206896551724\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the model\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix,accuracy_score\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First there is the confusion matrix, the first one is the training one and the second is the prediction. \n",
    "\n",
    "- 11 survival cases actually survived.\n",
    "- 2 dead cases were actually survivals.\n",
    "- 5 survival cases were actually deads.\n",
    "- 11 dead cases were actually deads.\n",
    "\n",
    "What this information is telling me is that we are good at predicting the dead cases, most of the ones that we predict are correct. In the other side, we make a decent survival prediction but it is not better than the deads one.\n",
    "\n",
    "Then the classification report is the piece of information that probably give us the most amount:\n",
    "\n",
    "- 69 % of predicted non-survivals were correct.\n",
    "- 85% of predicted survivals were correct.\n",
    "- 85% of non-survivals were predicted.\n",
    "- 69% of survivals were predicted. \n",
    "- 13 actual occurrences of the non-survival class.\n",
    "- 16 actual occurrences of the survival class.\n",
    "\n",
    "Accuracy score of 0.7586, basically 75% of the predictions made with the algorithm were correct. What is actually a great percentage considering the difficulty of the Random Forest. This is a good way to understand that the variables I picked for my algorithm were actually the most important ones (because it allows my algorithm to predict the following observations with a very good rate). Also, it is very clear that the other variables did influenciate the result at least in a small amount, even so, they are not really necesary for my analysis.\n",
    "\n",
    "Also, according to this, the survivals predictions were actually extremely high with at 85%. With this I can make the conclusion that the AGE, SEX, PCLASS & PORT were actually the MOST important factors in order to survive in the Titanic. Also that not all the persons who had the \"optimous\" conditions were able to survive, but this is just a fact of probability what makes completely sense. \n",
    "\n",
    "### - **kNN**\n",
    "\n",
    "I’m going to use the kNN (k nearest neighbors) algorithm. This one allows you to find the “k” nearest neighbors of a “X” object and based on that give it a classification. This meaning that it will clasify the specific observation in a class or another (in this case 0 or 1 for the survival variable) according with what it learned through the training dataset. \n",
    "\n",
    "The way this alrorigthm works is by calculating the Euclidean distance between the observations, finding the nearest neighbors and making a classification prediction. This is how the Orange Script works. In this case, I selected 7 as k so the alrogithm is 7NN. There are 3 clear steps in the alrogithm:\n",
    "    1. The “Euclidean_distance” function calculates de distance of the current object from the x one.\n",
    "    2. The “get_neighbors” one, sorts them and obtains the nearest neighbors based on the distance.\n",
    "    2. According to the neighbors, the diagnosis value gets predicted.\n",
    "\n",
    "In python code is as simple as these 3 functions. \n",
    "\n",
    "![System of kNN prediction, Orange](https://raw.githubusercontent.com/Abril-Barrera/Smart-Business-Systems/master/m10.PNG)\n",
    "\n",
    "For this I am using the testDataset with the \"survival\" variable errased so the kNN alrgothm can predicts it. If it does, it means the the system is reliable and it is actually very uniform, the greater the percentage of similarities is, the more uniform the dataset is. \n",
    "\n",
    "First of all I add the traning data set and the sample test (without the survival values) and also add the kNN algorithm to the training set.Then I use the prediction widget to predict the missing values in the sample dataset. \n",
    "\n",
    "![System of kNN prediction, Orange](https://raw.githubusercontent.com/Abril-Barrera/Smart-Business-Systems/master/m6.PNG)\n",
    "\n",
    "This is the result of the predictions, each observation of the test set gets assigned a value in the survival variable.\n",
    "\n",
    "![Predictions](https://raw.githubusercontent.com/Abril-Barrera/Smart-Business-Systems/master/m7.PNG)\n",
    "\n",
    "Now I want to know how good is the system working. To do so, I compare the predictions with the original values. As you can see in this excel table they are actually quite good, giving me a 72% similarity. It definitely could be higher but considering the number of the dimensions involved in the prediction this is actually a great result. Proving that there is a clear patron in the survival rate and because of it, it definitely can be correctly predicted. \n",
    "\n",
    "![Excel comparision](https://raw.githubusercontent.com/Abril-Barrera/Smart-Business-Systems/master/m8.PNG)\n",
    "\n",
    "This is not a perfect algorithm, since it can be influenced by the value of the vast majority of the training set. It is vital to use an adequate number for K while using the algorithm, not so small and no so big. I believe a good way to find it, is while trying to predict values of data that is already in the training data set. Then comparing it with the actual result and modifying the “K” according to the results. What I did, is try different Ks but I thought 7 was definitely a good number considering the size of my sample. Overall, kNN is a very useful algorithm that can work very well in different situations. In this case I don't believe it is the most accurate option but it is definitely worth trying. As you can see in this plot, the final result of values that kNN predicted are extremely similar to the training data set one.\n",
    "\n",
    "![Plot kNN 0/1 values predicted](https://raw.githubusercontent.com/Abril-Barrera/Smart-Business-Systems/master/m9.PNG)\n",
    "\n",
    "### - **Tree**\n",
    "\n",
    "It is a very simple way to make a prediction \n",
    "The tree visualization is one of my favorite, because it allows you to understand information so easily (with layers). Just by looking at the tree I can confirm that the hipotesis used in the enterly dataset is correct. Even so I am going to use a prediction to see if the results are accurate. \n",
    "\n",
    "My survival tree model is stating:\n",
    "\n",
    "1. Sex is the most important variable. Being a woman was the most important factor to survive.\n",
    "2. Pclass was the second most important factor. The better class you have, the more likely you were to survive.\n",
    "3. Age. Being a kid or 20-40 made you more likely to survive.\n",
    "\n",
    "\n",
    "![Survival tree model](https://raw.githubusercontent.com/Abril-Barrera/Smart-Business-Systems/master/m13.PNG)\n",
    "\n",
    "My non-survival tree mode is stating: \n",
    "\n",
    "1. Male are 80% most likely to die.\n",
    "2. Lower class people died the most.\n",
    "3. People of all kind of ages died, the most between 20-40 (there were more people from that range age).\n",
    "\n",
    "![Non-surival tree model](https://raw.githubusercontent.com/Abril-Barrera/Smart-Business-Systems/master/m15.PNG)\n",
    "\n",
    "Then by using the training data set + test data set Im going to predict the survival values. The Orange model looks like this:\n",
    "\n",
    "![Orange tree](https://raw.githubusercontent.com/Abril-Barrera/Smart-Business-Systems/master/m14.PNG)\n",
    "\n",
    "So for the previous statements I get an accuracy of 75% that I believe is a good rate. But I want to know what will happen if I remove the gender of the predictions if my hipotesis is true then the predictions must get worse. Furthermore I am going to try to predict the same values but first without the GENDER and then without the PCLASS. \n",
    "\n",
    "![Without gender & pclass variables](https://raw.githubusercontent.com/Abril-Barrera/Smart-Business-Systems/master/m16.PNG)\n",
    "\n",
    "What we can see here is very validating, if I remove the variable sex the predictions go down to 65%, if I remove the variable pclass they go down to 69%. What proves the hipotesis, gender was the most important variable, even so there is something really interesting here. Pclass is actually very near it, what is a bit surprising. Beside that the model is behaving exactly as predicted. \n",
    "\n",
    "\n",
    "### - **Linear regression**\n",
    "\n",
    "Linear regression is used for finding linear relationship between target and one or more predictors. There are two types of linear regression- Simple and Multiple. This python code (by using the sklearn library), allows me to calculate simple linear regression.But since I am aiming to calculate the multiple one, I will use the orange algorithm for practicality.\n",
    "\n",
    "Simple linear regression is useful for finding relationship between two continuous variables. One is predictor or independent variable and other is response or dependent variable. It looks for statistical relationship but not deterministic relationship. Relationship between two variables is said to be deterministic if one variable can be accurately expressed by the other.\n",
    "\n",
    "Linear regression was developed in the field of statistics and is studied as a model for understanding the relationship between input and output numerical variables, but has been borrowed by machine learning. It is both a statistical algorithm and a machine learning algorithm.\n",
    "\n",
    "![Simple linear regression Python](https://raw.githubusercontent.com/Abril-Barrera/Smart-Business-Systems/master/m18.PNG)\n",
    "\n",
    "In order to calculate the linear regresion I have to do a change to my dataset, I will make all of the values continuous to be able to work. This is how my dataset looks like now:\n",
    "\n",
    "![Variables for linear regression](https://raw.githubusercontent.com/Abril-Barrera/Smart-Business-Systems/master/m17.PNG)\n",
    "\n",
    "Then I have to create the following system to be able to be able to predict through the multiple linear regression algorithm.\n",
    "\n",
    "![Multiple linear regression with Orange](https://raw.githubusercontent.com/Abril-Barrera/Smart-Business-Systems/master/m19.PNG)\n",
    "\n",
    "The information I am going to obtain with the linear regression is the following: \n",
    "\n",
    "- Mean squared error\n",
    "- Root mean square error \n",
    "- Mean absolute error \n",
    "\n",
    "The mean absolute error is de distance between the training dataset object and the one predicted. It shouldn't be a big number if our information is reliable. \n",
    "\n",
    "This value of the MAE must be close to 0,if it does, it means the algorithm is working appropiatly. The closer it is to 0 the better it is. It is very important to visualize this values because they allow us to understand the information in a better way. This is how the MAE & MSE look graphically:\n",
    "\n",
    "![MAE](https://raw.githubusercontent.com/Abril-Barrera/Smart-Business-Systems/master/m22.PNG)\n",
    "\n",
    "![MSE](https://raw.githubusercontent.com/Abril-Barrera/Smart-Business-Systems/master/m21.PNG)\n",
    "\n",
    "So when applying the prediction through the linear regresion I obtain results that go from 0-1 but in this case the survival variable must be binary (because you either can be alive or death). So I had to fixt that.\n",
    "In this picture the predictions show values different than 0/1, I used excel in order to normalize this values (x>.5) = survives (x<=.5) =dies and then I was actually able to obtain a binary prediction. Once I did so, I calculated the MAE in order to see how big the error was. \n",
    "\n",
    "Mean Absolute Error = 0.34\n",
    "\n",
    "![Multiple linear regression with Orange](https://raw.githubusercontent.com/Abril-Barrera/Smart-Business-Systems/master/m20.PNG)\n",
    "\n",
    "This error is actually in a very decent rate. The error rate could definitely improve but considering linear regression is definitely not a good approach for the titanic dataset (because it is mostly formed of continous values). And this kind of algorithm is for datasets that contain a large range of values for each variable. Even so the result is considerably good.\n",
    "\n",
    "\n",
    "## 6. **Conclusion**\n",
    "\n",
    "As predicted in the hipotesis, the information I obtained during the explanatory analysis was consistent with it. I used various algorithms in order to find different patrons and to have a better understanding of the system I was analyzing. I believe the phase that was most useful in order to do the correct analysis was the exploratory one. It allowed me to completely catch the functionality of the dataset and be able to know the steps I should follow in order to do a proper analysis.\n",
    "\n",
    "Also, I believe the data preparation and cleaning was such an important part of the process, because if I would have done it different I probably would have gotten different results. I was very afraid of overfitting data but I think now I did sucess in that way. Also I understand there are many algorithms for machine learning. I learned as many as them as possible but in order to implement them, I had to have a much better understanding of how they were working. \n",
    "\n",
    "Furthermore, I realized how good the tool Orange is, because it allowed me to do a fast, efficient and functional analysis of the dataset without having to develop all kind of algorithms. I will defintely be working with Orange more in the near future. \n",
    "\n",
    "Finally I learned that the data analysis area is a an extremely interesting one. I have done mathematical modeling before and I loved it. I believe this is as fun as it, just instead of using pure maths we develop algorithms but the capacity and potential is the same. Being able to predict a fenomeno is one of the best things I have ever done in my universitary carreer and I am sure I will continue seeking for more knowledge in the field."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
